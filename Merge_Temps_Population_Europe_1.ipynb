{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87250c7f-0e5c-482c-99b4-cc5ed1aee4b5",
   "metadata": {},
   "source": [
    "# Extracting files from :\n",
    "\n",
    "    * Eurostat [Population]\n",
    "    \n",
    "    * ECAD     [Températures]\n",
    "    \n",
    "    ==> These 2 types of files were merged by upstream processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbeb7c73-dd28-43ec-8220-e8b4edfc8323",
   "metadata": {},
   "source": [
    "# Libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5623c31a-8ae2-4d30-b289-9ee823919368",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################\n",
    "##############################                                                                               ##############################\n",
    "##############################                             European Climate Assessment  &                    ##############################\n",
    "##############################                                     Eurostat                                  ##############################\n",
    "##############################                                 Updated 14/11/22                              ##############################\n",
    "###########################################################################################################################################\n",
    "\n",
    "###  librairies declaration\n",
    "import warnings\n",
    "import time\n",
    "import pandas                             as pd\n",
    "#import duckdb                             as db\n",
    "import missingno                          as missin\n",
    "\n",
    "import numpy                              as np\n",
    "lasource_de_donnees=\"Sources : European Climate Assessment & Dataset (ECAD) & Eurostat urb-cpop - Updated 14/11/22\"\n",
    "import plotly.express                     as px\n",
    "import plotly.graph_objects               as go\n",
    "\n",
    "from math import *\n",
    "import scipy.stats                        as st\n",
    "\n",
    "# missing value\n",
    "import missingno                          as msno # msno.bar(le_df) ou matrix(le_df) ou heatmap(le_df)\n",
    "\n",
    "from scipy.stats                          import chi2_contingency\n",
    "from sklearn.cluster                      import KMeans\n",
    "import seaborn                            as sns; sns.set()\n",
    "\n",
    "import matplotlib.pyplot                  as plt\n",
    "\n",
    "import statsmodels.api                    as sm\n",
    "import statsmodels.formula.api            as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn                              import preprocessing,cluster,decomposition,metrics  # normalisation\n",
    "\n",
    "# Pour la fonction biplot :\n",
    "import matplotlib                         as mpl\n",
    "import matplotlib.cm                      as cm\n",
    "from scipy.spatial                        import ConvexHull\n",
    "\n",
    "# Normalize data :\n",
    "from sklearn.preprocessing                import StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set max rows displayed for readability\n",
    "#pd.set_option(\"display.max_rows\", 23)\n",
    "# Configure display options\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "strdirectory=\"../Analyse Temperature Europe/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be002b1e-0fde-4ba2-b59a-06a7eb3ba8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################################################\n",
    "##############################                                                                               ##############################\n",
    "##############################                             Definition of functions                          ##############################\n",
    "##############################                                                                               ##############################\n",
    "###########################################################################################################################################\n",
    "\n",
    "def conversion_sexadecimale_en_decimal_degre(la_list):\n",
    "  \"\"\"\n",
    "    Geographic coordinates are often given in degrees (1/90 of a right angle), minutes of arc (1/60 of a degree), and seconds of arc\n",
    "    (1/60 of an arc minute), which is not a problem for computers that work in binary. However, computer scientists judge\n",
    "    sometimes the sexagesimal system is impractical to manipulate and, without going so far as to use the grades (the grade being 1/100 of a right angle),\n",
    "    prefer to convert minutes and seconds into decimal fractions of degrees (we commonly use in this case the term \"decimal degrees\", at the risk of \n",
    "    confusion with the ranks). General wording :\n",
    "    latitude (decimal degrees) = degrees + (minutes / 60) + (seconds / 3600)\n",
    "\n",
    "    This function :\n",
    "     latitude = sum(float(x) / 60 ** n for n, x in eGeographic coordinates are often given in degrees (1/90 of a right angle), minutes of arc (1/60 of a degree), and seconds of bow\n",
    "    (1/60 of an arc minute), which is not a problem for computers that work in binary. However, computer scientists judge\n",
    "    sometimes the sexagesimal system is impractical to manipulate and, without going so far as to use the grades (the grade being 1/100 of a right angle),\n",
    "    prefer to convert minutes and seconds into decimal fractions of degrees (we commonly use in this case the term \"decimal degreesnumerate(latitude[:-1].split('-'))) * (1 if 'N' in latitude[- 1] else -1)\n",
    "      longitude = sum(float(x) / 60 ** n for n, x in enumerate(longitude[:-1].split('-'))) * (1 if 'E' in longitude[-1] else - 1)\n",
    "  \"\"\"\n",
    "  l_row=pd.Series(la_list).iloc[0]\n",
    "  print(\"Element 0 :\",l_row[0],\"Element 1 :\",l_row[1],\"Element 2 :\",l_row[2])\n",
    "  #degree=pd.to_numeric(l_row[0]) #; minute =la_list[1].astype(int) ; seconde = la_list[2].astype(int)\n",
    "  la_valeur=pd.to_numeric(l_row[0],downcast='signed') + (pd.to_numeric(l_row[1],downcast='signed')/60) + (pd.to_numeric(l_row[2],downcast='signed')/3600)\n",
    "  print(\"La conversion donne :\",la_valeur)\n",
    "  return la_valeur\n",
    "\n",
    "\n",
    "def conversion_sexadecimale_en_decimal_degre_2(l_row):\n",
    "  ''' This allows you to convert from decimal to degree\n",
    "  '''\n",
    "  la_valeur=pd.to_numeric(l_row[0],downcast='signed') + (pd.to_numeric(l_row[1],downcast='signed')/60) + (pd.to_numeric(l_row[2],downcast='signed')/3600)\n",
    "  return round(la_valeur,4)\n",
    "\n",
    "def attribution_classe(x,df_classe_maitre):\n",
    "  ''' This allows you to assign a class to the data read.\n",
    "  '''  \n",
    "  index_classe=0\n",
    "  for index, row in df_classe_maitre.iterrows():\n",
    "    if (x > row['borne_inf_strict'] and x <= row['borne_sup_egale']):\n",
    "      index_classe=row['la_borne']\n",
    "  return index_classe\n",
    "\n",
    "def Saison_identifie(str_m,str_d):\n",
    "  ''' allows you to assign the same rule for season detection for all countries\n",
    "  '''    \n",
    "  val=\"\"\n",
    "  if (str_m=='12' or str_m=='01' or str_m=='02'):\n",
    "    val=\"hiver\"\n",
    "  if (str_m=='03' or str_m=='04' or str_m=='05'):\n",
    "    val=\"printemps\"\n",
    "  if (str_m=='06' or str_m=='07' or str_m=='08'):\n",
    "    val=\"ete\"\n",
    "  if (str_m=='09' or str_m=='10' or str_m=='11'):\n",
    "    val=\"automne\"\n",
    "  return val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3f70c8-ac92-423a-9957-81b8a5a326a2",
   "metadata": {},
   "source": [
    "# Declaration of dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8bdd28-162a-4011-aaf5-56778ec92643",
   "metadata": {},
   "source": [
    "Création de dictionnaire afin d'identifier le pays et de positionner son nom ainsi que chaque capitale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc29d857-3ce3-4986-ab34-7a1f81a7fb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "##   Insertion of country names for our study; this facilitates our understanding of graphs.\n",
    "les_noms={}\n",
    "les_noms[\"FR\"] = \"France\";les_noms[\"GB\"] = \"Royaume-Uni\";les_noms[\"IT\"] = \"Italie\";les_noms[\"LV\"] = \"Lettonie\"\n",
    "les_noms[\"EE\"] = \"Estonie\";les_noms[\"LT\"] = \"Lituanie\";les_noms[\"NL\"] = \"Pays-Bas\";les_noms[\"HU\"] = \"Hongrie\"\n",
    "les_noms[\"SE\"] = \"Suede\" ;les_noms[\"NO\"] = \"Norvege\";les_noms[\"RO\"] = \"Roumanie\"\n",
    "les_noms[\"PL\"] = \"Pologne\" ;les_noms[\"ES\"] = \"Espagne\";les_noms[\"AT\"] = \"Autriche\" ;les_noms[\"DE\"] = \"Allemagne\"\n",
    "les_noms[\"TR\"] = \"Turquie\"\n",
    "\n",
    "les_capitales={}\n",
    "les_capitales[\"PARIS\"]=\"O\";les_capitales[\"LONDON\"]=\"O\"\n",
    "les_capitales[\"RIGA\"]=\"O\";les_capitales[\"TALLINN\"]=\"O\"\n",
    "les_capitales[\"VILNIUS\"]=\"O\";les_capitales[\"AMSTERDAM\"]=\"O\"\n",
    "les_capitales[\"BUDAPEST\"]=\"O\";les_capitales[\"STOCKHOLM\"]=\"O\"\n",
    "les_capitales[\"OSLO\"]=\"O\";les_capitales[\"WARSZAWA\"]=\"O\"\n",
    "les_capitales[\"MADRID\"]=\"O\";les_capitales[\"WIEN\"]=\"O\"\n",
    "les_capitales[\"BERLIN\"]=\"O\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3da54-fc0a-4037-bbb7-3bb063227b68",
   "metadata": {},
   "source": [
    "# Merge files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114b226-12fe-4e79-a2d7-349ca89cf462",
   "metadata": {},
   "source": [
    "Merge files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53d9d4a0-9615-40ad-8911-59dded1b2bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> Durée de traitement de chargement fichier : 8.0 min 7.362792253494263 sec\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Start of extraction\n",
    "debut=time.time()\n",
    "\n",
    "# Initialize an empty DataFrame to store the concatenation\n",
    "df_fichier = pd.DataFrame()\n",
    "\n",
    "# Loop to read each file and concatenate its contents to the main DataFrame\n",
    "for cle in les_noms:\n",
    "    #print(\"Pays : \",cle)\n",
    "    df_temporaire = pd.read_excel(strdirectory+\"DecennieSourceWeather_ce_Pays_\"+cle+\".xlsx\")\n",
    "    df_fichier = pd.concat([df_fichier, df_temporaire], ignore_index=True)\n",
    "    \n",
    "fin=time.time()\n",
    "min=(fin-debut)//60\n",
    "sec=(fin-debut)-(min*60)\n",
    "print(\"---> Durée de traitement de chargement fichier :\",min,\"min\",sec,\"sec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c7abb4-9674-45b0-bbfc-d6cfe96fa561",
   "metadata": {},
   "source": [
    "# Pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ca9c7a-1545-4a71-a271-3d47d0823ce7",
   "metadata": {},
   "source": [
    "PRE PROCESSING - ETAPE 1  :\n",
    "    \n",
    "    Date format\n",
    "    \n",
    "    Rename\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e78c20-e4c1-41c5-a5bf-beab247efb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change date formatChange date format\n",
    "df_fichier['DATE_dt']=pd.to_datetime(df_fichier['DATE'],format=\"%Y%m%d\")\n",
    "#fichier['YEAR_dt']=fichier['DATE_dt'].dt.year\n",
    "df_fichier['YEAR_dt']=pd.to_datetime(df_fichier['DATE_dt'],format=\"%Y\")\n",
    "# recovery of years and months\n",
    "df_fichier['DATE_str_YEAR']=df_fichier['DATE'].astype(str).str[0:4]\n",
    "df_fichier['DATE_str_MONTH']=df_fichier['DATE'].astype(str).str[4:6]\n",
    "df_fichier['DATE_str_DAY']=df_fichier['DATE'].astype(str).str[6:8]\n",
    "## Correct the temperature value\n",
    "df_fichier['TEMPERATURE']=df_fichier['TX']*10**-1\n",
    "\n",
    "df_fichier.rename(columns={'Value_corrigee':'NB_HAB_BRUT','sans_virgule_corrigee':'NB_HAB','CITIES_clean':'CITY'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb6369a-57fb-4858-9024-f3f23e6d0736",
   "metadata": {},
   "source": [
    "PRE PROCESSING - ETAPE 2  :\n",
    "   \n",
    "    Interesting column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aace51dc-dce6-46e5-bd02-383010747a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column with uninteresting data for the rest of the analysis; to delete :\n",
    "df_fichier.drop(['PARNAME','TX','DATE','Q_TX','TIME','STAID_num','SOUID','NB_HAB_BRUT'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc1188b-7d2e-49fb-852b-fabca3cfb6ba",
   "metadata": {},
   "source": [
    "PRE PROCESSING - ETAPE 3 :\n",
    "    \n",
    "    Filter on data > 2002 ?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fb02df4-b02e-4174-850d-a985a129fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data recovery from 01/01/2002; LOWER TERMINAL FOR DATES\n",
    "'''\n",
    "df_fichier=df_fichier.query(\"YEAR_dt > 2002\")\n",
    "'''\n",
    "# Identifying the seasons\n",
    "df_fichier['Saison']=df_fichier.apply(lambda row : Saison_identifie(row['DATE_str_MONTH'],row['DATE_str_DAY']), axis = 1)\n",
    "#------------------------------------------------------------------------------------------------------------------------***\n",
    "# Conservation of the global file before processing\n",
    "df_fichier_brut=df_fichier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8959f325-cdb6-44a0-9ca0-b7cce5d456b6",
   "metadata": {},
   "source": [
    "PRE PROCESSING - ETAPE 4 :\n",
    "\n",
    "    Création de classe pour identifier les saisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b132924a-9e21-4a07-9660-8e0cad006664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying the seasons\n",
    "# df_fichier['Saison']=df_fichier.apply(lambda row : Saison_identifie(row['DATE_str_MONTH'],row['DATE_str_DAY']), axis = 1)\n",
    "\n",
    "#Conservation du fichier global avant traitement\n",
    "#df_fichier_brut=df_fichier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31cd87e-449c-45c6-9f2a-382e47698757",
   "metadata": {},
   "source": [
    "PRE PROCESSING - ETAPE 5 :\n",
    "\n",
    "    LAT, LON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "584a1e32-deb9-40c6-92e8-7e38ae178310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion en degré décimal 1/\n",
      "---> Conversion processing time : 15.0 min 30.821916580200195 sec\n"
     ]
    }
   ],
   "source": [
    "# Creating a column that breaks down latitude and longitude data\n",
    "df_fichier['LAT_Decompose']=df_fichier['LAT'].str.split(\":\")\n",
    "df_fichier['LON_Decompose']=df_fichier['LON'].str.split(\":\")\n",
    "\n",
    "deb=time.time()\n",
    "\n",
    "print(\"Conversion en degré décimal 1/\")\n",
    "# Objective of this block is to transform all the hexadecimal references into decimal degrees\n",
    "# Then insert them into a dictionary in order to read it to assign the correct transformation to the main file\n",
    "df_lon=df_fichier.groupby([\"LON\"],as_index=False).agg({'CN':'count'})\n",
    "df_lat=df_fichier.groupby([\"LAT\"],as_index=False).agg({'CN':'count'})\n",
    "# We give the same name to the column to prepare for concatenation\n",
    "df_lon.rename(columns={'LON':'lat_lon'},inplace=True)\n",
    "df_lat.rename(columns={'LAT':'lat_lon'},inplace=True)\n",
    "# Concatenation of values\n",
    "df_lat_lon=pd.concat([df_lat,df_lon],ignore_index=True)\n",
    "# uniqueness values\n",
    "df_lat_lon.groupby([\"lat_lon\"],as_index=False).agg({'CN':'count'})\n",
    "df_lat_lon['lat_lon_Decompose']=df_lat_lon['lat_lon'].str.split(\":\")\n",
    "df_lat_lon.drop(columns={'CN'},inplace=True)\n",
    "df_lat_lon['lat_lon_degre']=df_lat_lon.apply(lambda row : conversion_sexadecimale_en_decimal_degre_2(row['lat_lon_Decompose']),axis=1)\n",
    "\n",
    "# dictionary to save data frames\n",
    "le_dict={}\n",
    "\n",
    "for i in range(len(df_lat_lon)):\n",
    "  # Adding elements of a dictionary with le_dict[the key] = the value\n",
    "  le_dict[df_lat_lon.iloc[i,0]] = df_lat_lon.iloc[i,2]\n",
    "\n",
    "# We reconcile the data from the dictionaries and the df :\n",
    "# Data initialization\n",
    "lat=[];lon=[];noms=[];capitales=[]\n",
    "\n",
    "lat_cle='00:00:00';lon_cle='00:00:00';lat_val=0;lon_val=0\n",
    "cn_cle='';cn_val='inconnu';nom_cap='inconnu';nom_cap_cle=''\n",
    "for i in range(len(df_fichier)):\n",
    "  nom_cap_val='N'\n",
    "  if (lat_val ==0 and lon_val==0 and cn_val=='inconnu') or lat_cle!=df_fichier.iloc[i,5] or lon_cle!=df_fichier.iloc[i,6] :\n",
    "    lat_cle=df_fichier.iloc[i,3]; lon_cle=df_fichier.iloc[i,4] ; cn_cle=df_fichier.iloc[i,2]  ;nom_cap_cle=df_fichier.iloc[i,0]\n",
    "    for k in le_dict.keys():\n",
    "      if k==lat_cle:\n",
    "        lat_val=le_dict.get(k)\n",
    "      if k==lon_cle:\n",
    "        lon_val=le_dict.get(k)\n",
    "    for j in les_noms.keys():\n",
    "        if j==cn_cle:\n",
    "          cn_val=les_noms.get(j)\n",
    "    for l in les_capitales.keys():\n",
    "        if l==nom_cap_cle:\n",
    "          nom_cap_val=les_capitales.get(l)\n",
    "\n",
    "  lat.append(lat_val)\n",
    "  lon.append(lon_val)\n",
    "  noms.append(cn_val)\n",
    "  capitales.append(nom_cap_val)\n",
    "\n",
    "# We add the columns found to the df\n",
    "df_fichier['LAT_degre']=lat\n",
    "df_fichier['LON_degre']=lon\n",
    "df_fichier['CN_NAME']=noms\n",
    "df_fichier['CAPITALE']=capitales\n",
    "\n",
    "# Conservation of the global file before processing\n",
    "df_fichier_brut_convertit=df_fichier.copy(deep=True)\n",
    "\n",
    "# We clean the uninteresting columns\n",
    "df_fichier.drop(['LAT_Decompose','LON_Decompose'], axis=1, inplace=True)\n",
    "\n",
    "fin=time.time()\n",
    "min=(fin-debut)//60\n",
    "sec=(fin-debut)-(min*60)\n",
    "print(\"---> Conversion processing time :\",min,\"min\",sec,\"sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08628fb6-2546-4c97-86bd-b1b048891731",
   "metadata": {},
   "source": [
    "PRE PROCESSING - ETAPE 6 :\n",
    "\n",
    "    Removing duplicate cities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4e4da1ce-de7e-43a0-ae63-ba0f82ddb86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing duplicate cities (pls sensors) 2/\n"
     ]
    }
   ],
   "source": [
    "# Eviter les doublons sur même ville\n",
    "# A city must appear 1 time PER DAY OF TEMPERATURE TAKING, even if it has several sensors!!\n",
    "# It is therefore necessary to average the heights of the sensors, the temperatures, the latitude, the longitude and the number of inhabitants\n",
    "print(\"Removing duplicate cities (pls sensors) 2/\")\n",
    "\n",
    "fichier_LAT_LON_HGHT=df_fichier.groupby(['CITY','CN_NAME','CN','CAPITALE'],as_index=False).agg({'LAT_degre':'mean','LON_degre':'mean','HGHT_num':'mean'})\n",
    "#fichier_LON=df_fichier.groupby(['CITY','CN_NAME','CN','CAPITALE'],as_index=False).agg({'LON_degre':'mean'})\n",
    "#fichier_HGHT=df_fichier.groupby(['CITY','CN_NAME','CN','CAPITALE'],as_index=False).agg({'HGHT_num':'mean'})\n",
    "\n",
    "#fichier_LAT_LON=pd.merge(fichier_LAT,fichier_LON,on=['CITY','CN_NAME','CN','CAPITALE'],how='inn,er')\n",
    "#fichier_LAT_LON_HGHT=pd.merge(fichier_LAT_LON,fichier_HGHT,on=['CITY','CN_NAME','CN','CAPITALE'],how='inner')\n",
    "#fichier_LAT_LON_HGHT\n",
    "fichier_prep=pd.merge(df_fichier,fichier_LAT_LON_HGHT,on=['CITY','CN_NAME','CN','CAPITALE'],how='inner')\n",
    "\n",
    "fichier_prep.drop(['HGHT_num_x','LAT_degre_x','LON_degre_x'], axis=1, inplace=True)\n",
    "\n",
    "fichier_prep.rename(columns={'LAT_degre_y':'LAT_degre','LON_degre_y':'LON_degre','HGHT_num_y':'HGHT_num'},inplace=True)\n",
    "# Réorganiser les colonnes dans l'ordre souhaité\n",
    "nouvel_ordre_colonnes = ['CITY','CN', 'CN_NAME','CAPITALE','LAT','LON','Saison','LAT_degre','LON_degre','HGHT_num',\n",
    "                        'DATE_dt','YEAR_dt','DATE_str_YEAR','DATE_str_MONTH','DATE_str_DAY','TEMPERATURE', 'NB_HAB']\n",
    "fichier_prep=fichier_prep[nouvel_ordre_colonnes]\n",
    "\n",
    "fichier=fichier_prep.groupby([\"CITY\",\"CN_NAME\",\"CN\",\"CAPITALE\",\"Saison\",\"DATE_dt\",\"YEAR_dt\",\"DATE_str_YEAR\",\n",
    "        'DATE_str_MONTH','DATE_str_DAY'],as_index=False).agg({'LAT_degre':'mean',\n",
    "        'LON_degre':'mean','HGHT_num':'mean',\n",
    "        'TEMPERATURE':'mean','NB_HAB':'mean'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78030fa0-945a-4765-a8a2-90e1d961a69b",
   "metadata": {},
   "source": [
    "PRE PROCESSING - ETAPE 7 :\n",
    "\n",
    "    Delete observations if no density of inhabitants\n",
    "    Rounded figures on LAT and LON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70085b98-1b7b-42bf-b71e-4dfe5ab8de7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suppression des lignes pour lesquelles pas de données densité d'habitant 3/\n",
      "End of 1st part of treatment\n"
     ]
    }
   ],
   "source": [
    "# Objectif : retirer les lignes pour lesquelles le nombre d'habitants est null\n",
    "print(\"Suppression des lignes pour lesquelles pas de données densité d'habitant 3/\")\n",
    "# Identifies the lines for which the number of inhabitants is not specified\n",
    "df_city_cn_avec_hab_null=fichier.loc[fichier['NB_HAB'].isnull()].groupby([\"CITY\",\"CN\",'CN_NAME'],as_index=False).agg({'DATE_dt':'count'})\n",
    "df_city_cn_avec_hab_null['flag']='O'\n",
    "# Merge into left inner join using the flag column, I delete the left join result and I eliminate my lines having a zero number of inhabitants\n",
    "tempo=fichier.merge(df_city_cn_avec_hab_null,on=['CITY','CN','CN_NAME'] ,how='left').query(\"flag != 'O'\")\n",
    "tempo.drop(['flag','DATE_dt_y'], axis=1,inplace=True)\n",
    "tempo.rename(columns={'DATE_dt_x':'DATE_dt'},inplace=True)\n",
    "\n",
    "# We round so as not to have different lat and lon\n",
    "fichier['LAT_degre']=fichier['LAT_degre'].round(5)\n",
    "fichier['LON_degre']=fichier['LON_degre'].round(5)\n",
    "print(\"End of 1st part of treatment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24c2402a-a1a3-4e1f-a161-ce04b868b795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################                 ECRITURE                #################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4703f4a2-536f-45b3-8e94-29c282153579",
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier.to_csv(strdirectory+'Extraction_etape_1_data_temps_hab.csv', index=False,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d880f6a0-9297-4dcf-b34f-7573e1a50dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################                     FIN                 ################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd79a5-c91e-46f1-bf9a-317e551a4278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
